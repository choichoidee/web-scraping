{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = HTMLSession()\n",
    "content_page_url = 'http://www.passiontimes.hk/category/1/1/?page={}'\n",
    "content_url = 'http://www.passiontimes.hk/article/08-31-2018/{}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get content page num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_page_num():\n",
    "#     start_content_page = content_page_url.format(1)\n",
    "#     r = session.get(start_content_page)\n",
    "#     page_num = int(r.html.find('h5>span')[0].text.split('頁')[0].split('共')[1])\n",
    "#     assert type(page_num) == int\n",
    "#     return page_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract page id from single content page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(page_num):\n",
    "    r = session.get(content_page_url.format(page_num))\n",
    "    url_list = [tag.attrs['href'] for tag in r.html.find('div.textBox>h4>a[href*=\"www.passiontimes.hk\"]')]\n",
    "    return list(set(url_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_content(url):\n",
    "    dict_return = {}\n",
    "    r = session.get(url)\n",
    "    page_id = url.split('/')[-1]\n",
    "    dict_return['title'] = r.html.find('header>h1')[0].text\n",
    "    dict_return['category'] = r.html.find('div.page-path>h4>a')[-1].text\n",
    "    dict_return['content'] = ''.join([tag.text for tag in r.html.find('div.article-body>div,div.article-body>p')])\n",
    "    date = r.html.find('time.published')[0].attrs['datetime']\n",
    "    date = datetime.datetime.strptime(date,\"%m-%d-%Y\")\n",
    "    dict_return['date'] = date\n",
    "    dict_return['url'] = url\n",
    "    return dict_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Pages of a content page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_content(list_urls):\n",
    "    for url in list_urls:\n",
    "        yield get_single_content(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv writer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = ['title', 'category', 'content', 'date', 'url']\n",
    "def writer_initiator():\n",
    "    with open('scraper.csv', 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer(multi_content):\n",
    "    with open('scraper.csv','a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        while True:\n",
    "            try:\n",
    "                writer.writerow(next(multi_content))\n",
    "            except StopIteration:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_page = 1\n",
    "stop_page = 15 #total 680"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping Content Page 15.\r"
     ]
    }
   ],
   "source": [
    "writer_initiator()\n",
    "while True:\n",
    "    print('Scrapping Content Page {}.'.format(current_page), end=\"\\r\")\n",
    "    list_urls = get_ids(current_page)\n",
    "    articles_generator = multi_content(list_urls)\n",
    "    writer(articles_generator)\n",
    "    current_page += 1\n",
    "    if current_page == stop_page+1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
